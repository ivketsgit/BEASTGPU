{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbc2075-f494-4109-92ba-c470fd6789bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams.update({'font.size': 12})  # Change 14 to your desired font size\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import statistics\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b932af0-3ee2-4568-8d0f-6a02a09b2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    duration: float\n",
    "    runs: int\n",
    "    min: float\n",
    "    mean: float\n",
    "    max: float\n",
    "    std: float\n",
    "    median: float\n",
    "    percentile_75: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e221d5e0-fdb3-4fad-abff-0be5cd3eddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_paths(*paths):\n",
    "    return Path(paths[0]).joinpath(*paths[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7809730c-825e-4705-89fa-281de64a801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkSuite:\n",
    "    results: List[BenchmarkResult] = field(default_factory=list)\n",
    "\n",
    "    def add_result(self, result: BenchmarkResult):\n",
    "        self.results.append(result)\n",
    "\n",
    "    def average(self) -> BenchmarkResult:\n",
    "        count = len(self.results)\n",
    "        if count == 0:\n",
    "            raise ValueError(\"No results to average.\")\n",
    "        \n",
    "        return BenchmarkResult(\n",
    "            duration=sum(r.duration for r in self.results) / count,\n",
    "            runs=sum(r.runs for r in self.results) // count,\n",
    "            min=min(r.min for r in self.results),\n",
    "            mean=statistics.mean(r.mean for r in self.results),\n",
    "            max=max(r.max for r in self.results),\n",
    "            std=statistics.mean(r.std for r in self.results),\n",
    "            median=statistics.mean(r.median for r in self.results),\n",
    "            percentile_75=statistics.mean(r.percentile_75 for r in self.results),\n",
    "        )\n",
    "\n",
    "def parse_benchmark_file(text: str) -> BenchmarkSuite:\n",
    "    suite = BenchmarkSuite()\n",
    "\n",
    "    # Pattern to match each benchmark block\n",
    "    block_pattern = re.compile(\n",
    "        r\"Manual Benchmark of duration ([\\d.]+) over (\\d+) runs:\\s*\"\n",
    "        r\"Min: ([\\d.]+) s\\s*\"\n",
    "        r\"Mean: ([\\d.]+) s\\s*\"\n",
    "        r\"Max: ([\\d.]+) s\\s*\"\n",
    "        r\"Std: ([\\d.]+) s\\s*\"\n",
    "        r\"2nd Quartile \\(Median\\): ([\\d.]+) s\\s*\"\n",
    "        r\"3rd Quartile \\(75th percentile\\): ([\\d.]+) s\",\n",
    "        re.MULTILINE\n",
    "    )\n",
    "\n",
    "    for match in block_pattern.finditer(text):\n",
    "        result = BenchmarkResult(\n",
    "            duration=float(match.group(1)),\n",
    "            runs=int(match.group(2)),\n",
    "            min=float(match.group(3)),\n",
    "            mean=float(match.group(4)),\n",
    "            max=float(match.group(5)),\n",
    "            std=float(match.group(6)),\n",
    "            median=float(match.group(7)),\n",
    "            percentile_75=float(match.group(8)),\n",
    "        )\n",
    "        suite.add_result(result)\n",
    "\n",
    "    return suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c18e2fe2-2d2b-46fa-86ef-a7a1fb3475cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density_values: [24 30 33 36 38 40]\n",
      "intgral_amount: [ 764411904 1866240000 2732361984 3869835264 4804153344 5898240000]\n",
      "[18.73208503778125, 18.73208503778125, 42.637543187843754, 42.637543187843754, 58.890477992875, 58.890477992875, 85.0356678348125, 85.0356678348125, 96.34026951709374, 96.34026951709374, 102.313327849625, 102.313327849625]\n",
      "[6.603057326, 6.603057326, 30.209795602, 30.209795602, 36.455525738, 36.455525738, 59.920752235, 59.920752235, 60.870280942, 60.870280942, 63.991986622, 63.991986622]\n",
      "[22.276566082, 22.276566082, 48.098127016, 48.098127016, 64.642421383, 64.642421383, 97.123700754, 97.123700754, 103.600646364, 103.600646364, 134.160065047, 134.160065047]\n"
     ]
    }
   ],
   "source": [
    "path = \"BEASTGPU/data\"\n",
    "with open(merge_paths(path, \"graph_data.jl\"), \"r\") as f:\n",
    "    content = f.read()\n",
    "density_values = re.search(r'density_values\\s*=\\s*\\[([^\\]]+)\\]', content)\n",
    "intgral_amount = re.search(r'intgral_amount\\s*=\\s*\\[([^\\]]+)\\]', content)\n",
    "\n",
    "if density_values and intgral_amount:\n",
    "    density_list = np.array(list(map(int, density_values.group(1).split(','))))\n",
    "    intgral_list = np.array(list(map(int, intgral_amount.group(1).split(','))))\n",
    "    print(\"density_values:\", density_list)\n",
    "    print(\"intgral_amount:\", intgral_list)\n",
    "else:\n",
    "    print(\"One or both arrays not found.\")\n",
    "\n",
    "\n",
    "CPU_median = []\n",
    "CPU_min = []\n",
    "CPU_max = []\n",
    "\n",
    "for density_value in density_list:\n",
    "    with open(merge_paths(path, \"CPUMultiThread\",str(density_value),\"quadrule\" + \".txt\"), \"r\") as f:\n",
    "        content = f.read()\n",
    "    suite = parse_benchmark_file(content)\n",
    "    average_result = suite.average()\n",
    "    CPU_median.append(average_result.median)\n",
    "    CPU_min.append(average_result.min)\n",
    "    CPU_max.append(average_result.max)\n",
    "\n",
    "\n",
    "\n",
    "    CPU_median.append(average_result.median)\n",
    "    CPU_min.append(average_result.min)\n",
    "    CPU_max.append(average_result.max)\n",
    "\n",
    "\n",
    "\n",
    "print(CPU_median)\n",
    "print(CPU_min)\n",
    "print(CPU_max)\n",
    "\n",
    "\n",
    "GPU_median = []\n",
    "GPU_min = []\n",
    "GPU_max = []\n",
    "\n",
    "for density_value in density_list:\n",
    "    with open(merge_paths(path, \"GPU\",str(density_value),\"quadrule\" + \".txt\"), \"r\") as f:\n",
    "        content = f.read()\n",
    "    suite = parse_benchmark_file(content)\n",
    "    average_result = suite.average()\n",
    "    CPU_median.append(average_result.median)\n",
    "    CPU_min.append(average_result.min)\n",
    "    CPU_max.append(average_result.max)\n",
    "\n",
    "\n",
    "\n",
    "    GPU_median.append(average_result.median)\n",
    "    GPU_min.append(average_result.min)\n",
    "    GPU_max.append(average_result.max)\n",
    "\n",
    "\n",
    "print(GPU_median)\n",
    "print(CPU_min)\n",
    "print(CPU_max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5fbcd95-2ca7-4439-a21c-58725598da4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (832184483.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    ax.plot(memory, , marker='o', label='calculate on the GPU', color='blue')\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(, , marker='o', label='calculate on the GPU', color='blue')\n",
    "\n",
    "ax.set_xlabel('element pairs', fontsize=24)\n",
    "ax.set_ylabel('Execution \\nTime (s)', fontsize=24)\n",
    "#ax.set_title('Execution Time vs threads', fontsize=24)\n",
    "\n",
    "ax.tick_params(axis='both', labelsize=24)\n",
    "\n",
    "\n",
    "fig.canvas.draw()\n",
    "offset_text = ax.xaxis.get_offset_text()\n",
    "offset_text.set_size(24)  # Set font size of '1e9'\n",
    "\n",
    "plt.ylim(bottom=0)\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graphs/Quadrature_GPU_vs_CPU.svg\", format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6471b62e-5d8e-4b71-bf62-c81219add19c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
